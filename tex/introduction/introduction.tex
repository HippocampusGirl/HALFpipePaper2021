\section{Introduction}

The application of neuroimaging, in particular functional MRI (fMRI), has led to an explosion in knowledge about brain functions implicated in a range of human behaviors, cognitive processes, and emotions. Such research has been spurred by rapid advances in computationally intensive software required to perform complex algorithmic processing and statistical modeling of fMRI data. The resulting proliferation of software tools designed to fulfill various analytic functions has produced a large array of options for carrying out any given type of processing. Since the fMRI signal indirectly captures the neural processes of interest, a series of computational operations on fMRI data, referred to as the \term{analysis pipeline}, are necessary to arrive at interpretable results. In practice, each step is flexible and subject to a number of choices by the researcher, termed \term{analytic flexibility} \parencite{poldrack2017}. The steps in the analysis pipeline may be reordered, run with different parameters, or may be completely omitted in some cases. Understandably, users expect different tools performing the same function to generate (near) identical results when supplied with given input data. However, the multiplicity of tools has had the unintended consequence of generating inconsistent results from studies designed to answer the same research question, sometimes even when the same data is used as the starting point \parencite{botviniknezer2020}. Thus, \term{analytic flexibility} combined with the number of analysis steps, as well as the possible parameters for running each analysis step, has led to a vast multiplicity of methodologic variants and an equal number of possible results. This situation has contributed in part to what is widely hailed as a \term{crisis of reproducibility}, which now plagues the neuroimaging field \parencite{gorgolewski2016b,poldrack2017}.

One solution to improving reproducibility is to constrain the parameter space by limiting choices to default parameters established from empirically-derived best-practices \parencite{gruning2018}. For instance, established pipelines such as \soft{fMRIPrep} \parencite{esteban2019a} and \soft{C-PAC} \parencite{craddock2013} have automated many of these choices. An alternate approach is to run multiple analyses separately on the same input data with the same or different pipelines, but with different parameter selections for each analysis, and then compare the results. This second approach, termed \term{multiverse analysis} \parencite{steegen2016}, has the advantage that results from multiple analyses may be compared and alternate solutions may be presented in published reports to promote increased transparency. However, \term{multiverse analysis} has the disadvantage that it may ultimately not be possible to determine the optimal or even the correct solution, as true effects in non-simulated fMRI data are often unknown. 

The reproducibility crisis has led to an increased demand for standardized workflows to conduct both the preprocessing and postprocessing stages of fMRI analysis. The recent introduction and widespread adoption of standardized pipelines for fMRI data preprocessing has provided the research community with much-needed high-quality tools that have improved reproducibility \parencite{thompson2020b}. The four ingredients that are essential to data analysis and reproducible results are: (1) data and metadata availability, (2) code usage and transparency, (3) software installability, and (4) re-creation of the runtime environment. Relative to other processing pipelines, \soft{fMRIPrep} \parencite{esteban2019a} has grown in popularity due to its adoption of best practices, open-source availability, favorable user experience, and \term{glass-box} principles of transparency \parencite{poldrack2019}. However, \soft{fMRIPrep} is limited to the minimal preprocessing steps of fMRI data analysis, whilst variability in parameter selection for further preprocessing (e.g., data cleaning) and subsequent post-processing analysis steps (e.g., feature extraction, model specification) may compromise reproducibility.

The ENIGMA consortium has addressed the reproducibility crisis by pooling observational study data from structural and diffusion imaging (and more recently EEG and MEG), and by developing standardized pipelines, data harmonization methodology, and quality control protocols \parencite{thompson2020a}. These workflows have successfully analyzed structural and diffusion MRI data aggregated from large numbers of small- and medium-sized cohorts to accrue sufficient power to yield robust results on a wide range of neuropsychiatric conditions \parencite[e.g.][]{schmaal2020,vandenheuvel2020,hoogman2020}. However, until now the ENIGMA consortium has lacked the ability to reliably conduct consortium-wide analyses on fMRI data. More recently, however, the ENIGMA task-based \parencite{veer2019b} and resting-state fMRI \parencite{adhikari2019} working groups have spurred initiatives to bring the ENIGMA framework to the functional domain.

To support these initiatives within ENIGMA, we developed a standardized workflow that encompasses the essential elements of task-based and resting-state fMRI analyses from raw data to group-level statistics, builds on the progress and contributions of \soft{fMRIPrep} developers, and extends its functionality beyond preprocessing steps to include additional preprocessing, post-processing, and interactive tools for quality assessment. These extended features include: automatic and reliable conversion of fMRI data to BIDS format, spatial smoothing, temporal filtering, extended confounds regression, calculation of task-based activations, and resting-state feature extraction, including seed-based functional connectivity, network-template (dual) regression, atlas-based functional connectivity matrices, regional homogeneity (ReHo) analysis, and fractional amplitude of low frequency fluctuations (fALFF). Although each of these post-processing functions is available in other software packages and a few pipelines have incorporated a subset of these features, \soft{HALFpipe} combines all these post-processing tools from open-source neuroimaging packages with the preprocessing steps performed by \soft{fMRIPrep} (see Table~\ref{table:comparison}). Furthermore, although \soft{HALFpipe} provides recommended settings for each of the processing steps (see Table~\ref{table:settings}), it allows users to run any combinatorial number of these processing settings, thereby offering a streamlined infrastructure for pursuing multiverse analyses. Similar to other processing pipelines, \soft{HALFpipe} is available as a containerized image, thereby offering full control over the computational environment. In this article, we provide a detailed description of \soft{HALFpipe}. First we explain the software architecture and implementation, followed by a walkthrough of the procedure for running the software, and finally a discussion of the potential applications of the pipeline.

\input{./tab/comparison.tex}
