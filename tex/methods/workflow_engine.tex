\subsection{Workflow engine}

To obtain reproducible results, a core requirement for \soft{HALFpipe} was reproducible execution of the processing pipeline. As the ENIGMA consortium requires fMRI analysis of large datasets with several thousand samples, \soft{HALFpipe} was designed to parallelize processing on multiple computers or processor cores. Both of these specifications were achieved by implementation in \soft{Nipype}, \term{NeuroImaging in Python: Pipelines and Interfaces} \parencite{gorgolewski2011}. \soft{Nipype} is a workflow engine for neuroimaging that constructs an acyclic directed graph, in which nodes represent processing commands that need to be executed (the steps of the pipeline), while the edges represent inputs and outputs being passed between nodes (images or text files). In this formalization of a neuroimaging pipeline as a graph, the fastest order for execution across multiple processor cores can be determined.

The workflow graphs are modular and scalable, which means they can be nested and extended. \soft{HALFpipe} uses the workflows defined by \soft{fMRIPrep} and then connects these outputs to additional workflows. \soft{fMRIPrep} itself is modular and divided into multiple workflows: \soft{sMRIPrep} \parencite{esteban2021b}, \soft{SDCFlows} \parencite{esteban2020a}, \soft{NiWorkflows} \parencite{esteban2021a}, and \soft{NiTransforms} \parencite{goncalves2021}. The workflow graph facilitates saving and verifying intermediate results, and supports the user's ability to stop and later restart processing. \soft{HALFpipe} also uses the graphs to determine which intermediate results files are not needed by subsequent commands by using a tracing garbage collection algorithm \parencite{dijkstra1978}. As such, intermediate files do not accumulate on the storage device. This feature is implemented as a plugin to \soft{Nipype}.

\soft{Nipype} forms the basis of \soft{fMRIPrep} and \soft{C-PAC}, which are widely used in the neuroimaging community. However, it has several limitations that are relevant in the context of \soft{HALFpipe}. \soft{HALFpipe} is able to calculate features and statistical maps with different variations of preprocessing settings. To do this efficiently, intermediate results need to be re-used whenever possible. An improved second version of \soft{Nipype} is currently being developed, called \soft{Pydra} \parencite{jarecka2020}, which will be able to automatically detect repetitive processing commands, and automatically re-use outputs. Presently, until \soft{Pydra} becomes available, \soft{HALFpipe} calculates a four-letter hash code that uniquely identifies each pre-processing step. Before constructing a new pre-processing command, \soft{HALFpipe} checks whether its hash has already been added to the graph. If present, the existing command is re-used, significantly reducing processing times in the context of multiverse analysis or pipeline comparison (see Table~\ref{table:runtimes}).

\input{./tab/runtimes.tex}

A key requirement of \soft{HALFpipe} was robust and flexible handling of missing data. For instance a missing functional scan or statistical map does not cause \soft{HALFpipe} to fail. Additionally, \soft{HALFpipe} defines inclusion and exclusion criteria for scans, such as the maximum allowed motion (mean framewise displacement) or a minimum brain coverage when extracting a brain region's average signal. Finally, depending on the data set, statistical maps may need to be aggregated across runs or sessions within single subjects before a group-level model can be run. This means that the static graph has to be modified dynamically to adapt to the results of processing. \soft{HALFpipe} solves this problem by defining a data structure that not only contains the file names of statistical maps, but also the tags and metadata that can be used to adjust processing on the fly. For example, using this data structure, design matrices can be constructed for group models based on the actual subjects that have statistical maps available.
