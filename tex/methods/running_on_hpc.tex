\subsection{Running on a high-performance cluster}

Deploying \soft{Nipype} to perform computations on multiple nodes, such as on a high performance cluster (HPC) is particularly challenging. By default, \soft{Nipype} submits a separate job to the cluster queue for each processing command (graph node) regardless of the amount of time required to execute the command. A watcher process running on the head node collects outputs from completed commands and submits the next processing command. This process can be inefficient on some HPCs because computational resources need to be allocated and deallocated continually. We implemented a more efficient approach for \soft{HALFpipe} that partitions the processing graph into many independent subgraphs, which the user may submit as separate jobs. The smallest granularity available is one subgraph per subject that is invoked automatically with the command line flag \code{--use-cluster}. A \soft{Nipype} workflow is created and validated for all subjects before the pipeline starts running. In a cluster setting, the most efficient resource utilization is to submit each subject as a separate job and to run each job on two CPU cores.
